# Tensorflow SavedModel inferencing with NodeJS

This project showcases the differenent features and capabilities of Open Horizon to manage and deploy applications to edge devices.  We will walk through the process of registering a node agent on different edge devices using hzn (Open Horizon) CLI and perform inferencing/object detection by using the TF SavedModel that was trained and generated by tfjs-pipeline https://github.com/playground/tfjs-pipelline.  We will also leverage MMS(Model Management System) to publish new/udpate models to the management hub.  If an agent is registered and subscribed to MMS, it will periodically poll for updates.  If there is an update available, MMS will download the new/updated model and make it available for the agent.  The application running on the agent will pick up load in the new/updated model.  And if for some reason the application is not able to load in the new model due to bad or buggy deployment, the system will roll back and continue running with the current working model.


## Build docker image
npm run build-image --username=playbox21 --imagename=testimage

docker cp  /Users/jeff/Downloads/demo-model/version_2/. pensive_keller:server/data-set/
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md

docker run -it --cpu-period=100000 --cpu-quota=50000 --rm tfjs-pipeline
docker run -it --memory="6g" --memory-swap="40g" --memory-swappiness="100" --rm tfjs-pipeline
npm run pre_process --task=train_model --pipeline_config_path=/server/data-set/ssd_efficientdet_d0_512x512_coco17_tpu-8.config --model_dir=/server/data-set/training
npm run pre_process --task=build_all --image_dir=/server/data-set --origin=maximo
docker build --ssh github=$HOME/.ssh/id_rsa -t tfjs-pipeline-ubuntu-2104 .

sudo apt-get update && sudo apt-get install -y curl
curl -sL https://deb.nodesource.com/setup_16.x  | sudo bash
sudo apt-get -yq install nodejs
sudo apt-get install fswebcam -y 

### Nvidia
sudo docker run --rm --gpus all nvidia/cuda:10.0-base nvidia-smi
sudo apt-get install -y nvidia-docker2
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)    && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -    && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
curl https://get.docker.com | sh   && sudo systemctl --now enable docker
docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu
docker run --gpus all --rm nvidia/cuda nvidia-smi

export LD_LIBRARY_PATH=/usr/local/cuda-10.2/targets/aarch64-linux/lib/:$LD_LIBRARY_PATH
locate libcudart.so
sudo updatedb
sudo apt install mlocate -y
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
ls -l /usr/local/cuda/lib64
dpkg -l | grep cudart